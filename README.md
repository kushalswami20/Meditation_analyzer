````markdown
# üßò Meditation Analyzer

A deep learning-based system that analyzes a video input to determine whether a person is meditating. It combines insights from facial expressions, emotional state (vision + audio), body posture, and breathing rhythm using specialized neural network models.

---

## üîç Overview

This project leverages multiple deep learning models working in parallel to classify meditation status from video (and optionally audio) input. Each model is responsible for analyzing a different aspect of the human state:

- **Face Expression** ‚Äî Calm vs Not-Calm
- **Emotion (Vision & Audio)** ‚Äî Emotionally relaxed or stressed
- **Posture** ‚Äî Whether the meditation posture is correct
- **Breathing** ‚Äî Regular and deep breathing patterns

<img width="3940" height="2970" alt="ec7d26e2" src="https://github.com/user-attachments/assets/d2a9f018-3978-44dc-8a89-60934511da92" />
All outputs are combined using a **Decision Fusion System** to make a final meditation prediction.

---

## üß† Deep Learning Models Used

| Component           | Model Architecture                   | Task                                      |
|---------------------|--------------------------------------|-------------------------------------------|
| Face Expression     | MobileNet-V3-Small (CNN)             | Calm/Not-Calm facial classification       |
| Emotion (Vision)    | ResNet-18 + Squeeze-Excite           | Emotion detection from facial cues        |
| Emotion (Audio)     | 3-layer Bidirectional GRU            | Emotion detection from audio              |
| Posture             | 2-layer Bi-LSTM + Attention          | Posture classification from body keypoints|
| Breathing           | 1D CNN                               | Breathing regularity detection            |
| Fusion              | Weighted Average / Logistic Regression | Final meditation prediction              |

---

## üìà Training Performance

Each model was trained independently and evaluated on both training and validation data. Below are visualizations of the training progress over 100 epochs.

### ‚úÖ Accuracy Graphs

<img width="2400" height="1600" alt="72034469" src="https://github.com/user-attachments/assets/9fcc34de-9ab1-49ce-b966-96072b87cb92" />

- **Face Expression Accuracy**:  
  - Training: ~95%  
  - Validation: ~88%

- **Emotional Analyzer Accuracy**:  
  - Training: ~94%  
  - Validation: ~87%

---

### üìâ Loss Graphs

- **Posture Analyzer Loss**: Dropped from **1.6 to <0.1**
- **Breathing Analyzer Loss**: Training and validation loss dropped from **~1.8 to <0.1**

---

## üöÄ How to Run

1. Clone this repository:
   ```bash
   git clone https://github.com/your-username/meditation-analyzer.git
   cd meditation-analyzer
````

2. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

3. Run model training (example for Face Expression model):

   ```bash
   python train_face_expression.py
   ```

4. Run full prediction on a video:

   ```bash
   python predict_meditation.py --video input.mp4
   ```

---

## üìÇ Folder Structure

```
meditation-analyzer/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ face_expression/
‚îÇ   ‚îú‚îÄ‚îÄ emotion/
‚îÇ   ‚îú‚îÄ‚îÄ posture/
‚îÇ   ‚îî‚îÄ‚îÄ breathing/
‚îú‚îÄ‚îÄ data/
‚îú‚îÄ‚îÄ utils/
‚îú‚îÄ‚îÄ screenshots/
‚îÇ   ‚îú‚îÄ‚îÄ accuracy_graphs.png
‚îÇ   ‚îî‚îÄ‚îÄ loss_graphs.png
‚îú‚îÄ‚îÄ train_face_expression.py
‚îú‚îÄ‚îÄ predict_meditation.py
‚îî‚îÄ‚îÄ README.md
```

---

## ‚ú® Future Work

* Integrate real-time webcam inference.
* Add lightweight deployment via ONNX or TensorFlow Lite.
* Build a web dashboard to visualize inference results and feedback.

---

## üë§ Author

**Kushal Arogyaswami**
*B.Tech IT, Delhi Technological University*

---

## üìú License

MIT License ‚Äî use freely with credit.

```

---

Would you like me to generate placeholder images for the graphs or a badge-style project summary for the top of the README?
```
